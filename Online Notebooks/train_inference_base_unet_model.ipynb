{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":4905664,"datasetId":2845019,"databundleVersionId":4973231},{"sourceType":"modelInstanceVersion","sourceId":756800,"databundleVersionId":15766628,"modelInstanceId":578049,"modelId":590386}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":24043.285081,"end_time":"2026-02-20T16:32:07.512226","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-20T09:51:24.227145","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, re, glob, random, time\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport nibabel as nib","metadata":{"execution":{"iopub.status.busy":"2026-02-24T12:51:17.383988Z","iopub.status.idle":"2026-02-24T12:51:17.384235Z","shell.execute_reply.started":"2026-02-24T12:51:17.384109Z","shell.execute_reply":"2026-02-24T12:51:17.384123Z"},"papermill":{"duration":6.640511,"end_time":"2026-02-20T09:51:34.954220","exception":false,"start_time":"2026-02-20T09:51:28.313709","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download dataset\npath = kagglehub.dataset_download(\"javariatahir/litstrain-val\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T09:47:44.366708Z","iopub.execute_input":"2026-02-24T09:47:44.367282Z","iopub.status.idle":"2026-02-24T09:47:45.065348Z","shell.execute_reply.started":"2026-02-24T09:47:44.367254Z","shell.execute_reply":"2026-02-24T09:47:45.064637Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/litstrain-val\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -----------------------\n# 1) Config\n# -----------------------\n@dataclass\nclass CFG:\n    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    BASE_PATH: str = \"/kaggle/input/litstrain-val/LiTS(train_test)\"\n    TRAIN_CT_DIR: str = \"train_CT\"\n    TRAIN_MSK_DIR: str = \"train_mask\"\n\n    # LiverTumor segmentation cache\n    CACHE_DIR: str = \"/kaggle/working/lits_cache_LiverTumor256\"\n\n    IMG_SIZE: int = 256\n\n    VAL_SPLIT: float = 0.2\n    SEED: int = 42\n    HU_MIN: int = -100\n    HU_MAX: int = 400\n\n    VAL_SPLIT: float = 0.2\n    SEED: int = 42\n\n    KEEP_NEGATIVE_PROB: float = 0.05\n    ONLY_TUMOR_SLICES: bool = True\n\n    BATCH_SIZE: int = 16\n    NUM_WORKERS: int = 2\n    PIN_MEMORY: bool = True\n\ncfg = CFG()\nrandom.seed(cfg.SEED)\nnp.random.seed(cfg.SEED)\ntorch.manual_seed(cfg.SEED)\n\nos.makedirs(cfg.CACHE_DIR, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:45.066335Z","iopub.execute_input":"2026-02-24T09:47:45.066840Z","iopub.status.idle":"2026-02-24T09:47:45.126526Z","shell.execute_reply.started":"2026-02-24T09:47:45.066801Z","shell.execute_reply":"2026-02-24T09:47:45.125956Z"},"papermill":{"duration":0.069769,"end_time":"2026-02-20T09:51:35.029779","exception":false,"start_time":"2026-02-20T09:51:34.960010","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -----------------------\n# 2) Pairing helpers (same approach)\n# -----------------------\ndef extract_id(path: str) -> int:\n    m = re.search(r\"-(\\d+)\\.nii\", os.path.basename(path))\n    if not m:\n        raise ValueError(f\"Cannot parse id from {path}\")\n    return int(m.group(1))\n\ntrain_ct_paths = sorted(glob.glob(os.path.join(cfg.BASE_PATH, cfg.TRAIN_CT_DIR, \"volume-*.nii\")))\ntrain_msk_paths = sorted(glob.glob(os.path.join(cfg.BASE_PATH, cfg.TRAIN_MSK_DIR, \"segmentation-*.nii\")))\n\nct_map = {extract_id(p): p for p in train_ct_paths}\nmsk_map = {extract_id(p): p for p in train_msk_paths}\nids = sorted(list(set(ct_map.keys()) & set(msk_map.keys())))\nassert len(ids) > 0, \"No matched CT/mask pairs found.\"\n\nprint(f\"Matched volumes: {len(ids)}\")\n\ntrain_ids, val_ids = train_test_split(ids, test_size=cfg.VAL_SPLIT, random_state=cfg.SEED)\ntrain_ids = sorted(train_ids)\nval_ids = sorted(val_ids)\nprint(f\"Train volumes: {len(train_ids)} | Val volumes: {len(val_ids)}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:45.128073Z","iopub.execute_input":"2026-02-24T09:47:45.128634Z","iopub.status.idle":"2026-02-24T09:47:45.172289Z","shell.execute_reply.started":"2026-02-24T09:47:45.128600Z","shell.execute_reply":"2026-02-24T09:47:45.171681Z"},"papermill":{"duration":0.023292,"end_time":"2026-02-20T09:51:35.058354","exception":false,"start_time":"2026-02-20T09:51:35.035062","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Matched volumes: 111\nTrain volumes: 88 | Val volumes: 23\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -----------------------\n# 3) Preprocess helpers\n# -----------------------\ndef hu_window_and_scale(img2d: np.ndarray, hu_min: int, hu_max: int) -> np.ndarray:\n    x = np.clip(img2d, hu_min, hu_max).astype(np.float32)\n    x = (x - hu_min) / float(hu_max - hu_min)\n    return x\n\ndef resize2d(img2d: np.ndarray, size: int, is_mask: bool) -> np.ndarray:\n    interp = cv2.INTER_NEAREST if is_mask else cv2.INTER_AREA\n    return cv2.resize(img2d, (size, size), interpolation=interp)\n\ndef cache_paths(split: str):\n    img_dir = os.path.join(cfg.CACHE_DIR, split, \"images\")\n    msk_dir = os.path.join(cfg.CACHE_DIR, split, \"masks\")\n    os.makedirs(img_dir, exist_ok=True)\n    os.makedirs(msk_dir, exist_ok=True)\n    return img_dir, msk_dir","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:47.103105Z","iopub.execute_input":"2026-02-24T09:47:47.103809Z","iopub.status.idle":"2026-02-24T09:47:47.109265Z","shell.execute_reply.started":"2026-02-24T09:47:47.103779Z","shell.execute_reply":"2026-02-24T09:47:47.108476Z"},"papermill":{"duration":0.012997,"end_time":"2026-02-20T09:51:35.076545","exception":false,"start_time":"2026-02-20T09:51:35.063548","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_multi_channel_image(ct_slice: np.ndarray) -> np.ndarray:\n    # ensure float32\n    ct_slice = ct_slice.astype(np.float32, copy=False)\n\n    # Channel 1: liver window (40..400)\n    ch1 = np.clip(ct_slice, 40, 400)\n    ch1 = (ch1 - 40) / (400 - 40)\n\n    # Channel 2: soft tissue window (-100..200)\n    ch2 = np.clip(ct_slice, -100, 200)\n    ch2 = (ch2 + 100) / (200 + 100)\n\n    # Robust normalize for gradient channel\n    p1, p99 = np.percentile(ct_slice, (1, 99))\n    base = np.clip(ct_slice, p1, p99).astype(np.float32, copy=False)\n    base = (base - base.min()) / (base.max() - base.min() + 1e-8)\n\n    # IMPORTANT: make contiguous float32 for OpenCV\n    base = np.ascontiguousarray(base, dtype=np.float32)\n\n    sobelx = cv2.Sobel(base, ddepth=cv2.CV_32F, dx=1, dy=0, ksize=3)\n    sobely = cv2.Sobel(base, ddepth=cv2.CV_32F, dx=0, dy=1, ksize=3)\n    grad = cv2.magnitude(sobelx, sobely)\n    grad = grad / (grad.max() + 1e-8)\n\n    x = np.stack([ch1, ch2, grad], axis=-1).astype(np.float32)  # [H,W,3]\n    x = np.clip(x, 0, 1)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:49.469553Z","iopub.execute_input":"2026-02-24T09:47:49.469870Z","iopub.status.idle":"2026-02-24T09:47:49.476618Z","shell.execute_reply.started":"2026-02-24T09:47:49.469842Z","shell.execute_reply":"2026-02-24T09:47:49.475917Z"},"papermill":{"duration":0.013767,"end_time":"2026-02-20T09:51:35.095450","exception":false,"start_time":"2026-02-20T09:51:35.081683","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# -----------------------\n# 5) Fast dataset\n# -----------------------\nclass NPYSliceDataset(Dataset):\n    def __init__(self, cache_root: str, split: str, augment: bool = False, show_scan_progress: bool = True):\n        self.img_dir = os.path.join(cache_root, split, \"images\")\n        self.msk_dir = os.path.join(cache_root, split, \"masks\")\n\n        # tqdm during scan so you see something happening even on slow FS\n        img_glob = os.path.join(self.img_dir, \"*.npy\")\n        img_paths = glob.glob(img_glob)\n\n        if show_scan_progress:\n            # just to show progress, we iterate once (cost is small)\n            self.img_paths = []\n            for p in tqdm(sorted(img_paths), desc=f\"Indexing {split} .npy slices\"):\n                self.img_paths.append(p)\n        else:\n            self.img_paths = sorted(img_paths)\n\n        assert len(self.img_paths) > 0, f\"No cached images found in {self.img_dir}\"\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def _augment_pair(self, img, msk):\n        if random.random() < 0.5:\n            img = np.fliplr(img).copy()\n            msk = np.fliplr(msk).copy()\n        if random.random() < 0.5:\n            img = np.flipud(img).copy()\n            msk = np.flipud(msk).copy()\n        return img, msk\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        stem = os.path.basename(img_path)\n        msk_path = os.path.join(self.msk_dir, stem)\n        img = np.load(img_path).astype(np.float32)  # [H,W,3]\n        msk = np.load(msk_path).astype(np.float32)  # [H,W]\n        \n        if self.augment:\n            img, msk = self._augment_pair(img, msk)\n        \n        img_t = torch.from_numpy(img).permute(2,0,1)  # [3,H,W]\n        msk_t = torch.from_numpy(msk).unsqueeze(0)    # [1,H,W]\n        return img_t, msk_t","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:51.241455Z","iopub.execute_input":"2026-02-24T09:47:51.242157Z","iopub.status.idle":"2026-02-24T09:47:51.250244Z","shell.execute_reply.started":"2026-02-24T09:47:51.242129Z","shell.execute_reply":"2026-02-24T09:47:51.249449Z"},"papermill":{"duration":0.016773,"end_time":"2026-02-20T09:51:35.139986","exception":false,"start_time":"2026-02-20T09:51:35.123213","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ============================================================\n# 6) Base U-Net\n# ============================================================\nclass DoubleConvNoBN(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x): return self.net(x)\n\nclass UNetBase(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1, base=64):\n        super().__init__()\n        self.enc1 = DoubleConvNoBN(in_channels, base)\n        self.pool1 = nn.MaxPool2d(2)\n\n        self.enc2 = DoubleConvNoBN(base, base*2)\n        self.pool2 = nn.MaxPool2d(2)\n\n        self.enc3 = DoubleConvNoBN(base*2, base*4)\n        self.pool3 = nn.MaxPool2d(2)\n\n        self.enc4 = DoubleConvNoBN(base*4, base*8)\n        self.pool4 = nn.MaxPool2d(2)\n\n        self.bot = DoubleConvNoBN(base*8, base*16)\n\n        self.up4 = nn.ConvTranspose2d(base*16, base*8, 2, stride=2)\n        self.dec4 = DoubleConvNoBN(base*16, base*8)\n\n        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n        self.dec3 = DoubleConvNoBN(base*8, base*4)\n\n        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.dec2 = DoubleConvNoBN(base*4, base*2)\n\n        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.dec1 = DoubleConvNoBN(base*2, base)\n\n        self.outc = nn.Conv2d(base, out_channels, 1)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(self.pool1(x1))\n        x3 = self.enc3(self.pool2(x2))\n        x4 = self.enc4(self.pool3(x3))\n        xb = self.bot(self.pool4(x4))\n\n        u4 = self.up4(xb)\n        d4 = self.dec4(torch.cat([u4, x4], dim=1))\n\n        u3 = self.up3(d4)\n        d3 = self.dec3(torch.cat([u3, x3], dim=1))\n\n        u2 = self.up2(d3)\n        d2 = self.dec2(torch.cat([u2, x2], dim=1))\n\n        u1 = self.up1(d2)\n        d1 = self.dec1(torch.cat([u1, x1], dim=1))\n\n        return self.outc(d1)","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:53.675372Z","iopub.execute_input":"2026-02-24T09:47:53.675678Z","iopub.status.idle":"2026-02-24T09:47:53.686399Z","shell.execute_reply.started":"2026-02-24T09:47:53.675652Z","shell.execute_reply":"2026-02-24T09:47:53.685581Z"},"papermill":{"duration":0.019178,"end_time":"2026-02-20T09:51:35.164709","exception":false,"start_time":"2026-02-20T09:51:35.145531","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ============================================================\n# 7) Loss + Metrics\n# ============================================================\ndef dice_loss_from_logits(logits, y, eps=1e-6):\n    p = torch.sigmoid(logits)\n    num = 2*(p*y).sum(dim=(2,3)) + eps\n    den = (p+y).sum(dim=(2,3)) + eps\n    return 1 - (num/den).mean()\n\nclass BCEDice(nn.Module):\n    def __init__(self, pos_weight=1.0, bce_weight=0.5):\n        super().__init__()\n        self.register_buffer(\"pw\", torch.tensor([pos_weight], dtype=torch.float32))\n        self.bce_weight = bce_weight\n    def forward(self, logits, y):\n        bce = F.binary_cross_entropy_with_logits(logits, y, pos_weight=self.pw)\n        d   = dice_loss_from_logits(logits, y)\n        return self.bce_weight*bce + (1-self.bce_weight)*d\n\n@torch.no_grad()\ndef estimate_pos_weight(loader, device, max_batches=40):\n    pos = 0.0\n    neg = 0.0\n    for i, (x, y) in enumerate(loader):\n        if i >= max_batches: break\n        y = y.to(device)\n        pos += y.sum().item()\n        neg += (1 - y).sum().item()\n    return 1.0 if pos < 1 else float(neg / pos)\n\n@torch.no_grad()\ndef metrics_from_logits(logits, y, thr=0.5, eps=1e-8):\n    p = (torch.sigmoid(logits) > thr).float()\n    tp = (p*y).sum().item()\n    tn = ((1-p)*(1-y)).sum().item()\n    fp = (p*(1-y)).sum().item()\n    fn = ((1-p)*y).sum().item()\n    dice = (2*tp)/(2*tp+fp+fn+eps)\n    iou  = tp/(tp+fp+fn+eps)\n    acc  = (tp+tn)/(tp+tn+fp+fn+eps)\n    sens = tp/(tp+fn+eps)\n    spec = tn/(tn+fp+eps)\n    return {\"dice\":dice,\"iou\":iou,\"acc\":acc,\"sens\":sens,\"spec\":spec}\n\n@torch.no_grad()\ndef evaluate(model, loader, loss_fn, device):\n    model.eval()\n    agg = {\"loss\":0.0,\"dice\":0.0,\"iou\":0.0,\"acc\":0.0,\"sens\":0.0,\"spec\":0.0}\n    n = 0\n    loop = tqdm(loader, desc=\"Validation\", leave=False)\n    for x, y in loop:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n        logits = model(x)\n        loss = loss_fn(logits, y).item()\n        m = metrics_from_logits(logits, y)\n        bs = x.size(0)\n        agg[\"loss\"] += loss*bs\n        for k in m: agg[k] += m[k]*bs\n        n += bs\n        loop.set_postfix(loss=f\"{loss:.4f}\", dice=f\"{m['dice']:.3f}\", acc=f\"{m['acc']:.3f}\")\n    for k in agg: agg[k] /= max(n,1)\n    return agg\n\ndef train_one_epoch(model, loader, opt, loss_fn, device):\n    model.train()\n    running = 0.0\n    n = 0\n    loop = tqdm(loader, desc=\"Training\", leave=False)\n    for x, y in loop:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        opt.zero_grad(set_to_none=True)\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n\n        bs = x.size(0)\n        running += loss.item()*bs\n        n += bs\n\n        # show live loss + accuracy/dice for the current batch\n        m = metrics_from_logits(logits.detach(), y.detach())\n        loop.set_postfix(loss=f\"{loss.item():.4f}\", dice=f\"{m['dice']:.3f}\", acc=f\"{m['acc']:.3f}\")\n\n    return running/max(n,1)","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:47:56.131124Z","iopub.execute_input":"2026-02-24T09:47:56.131794Z","iopub.status.idle":"2026-02-24T09:47:56.146121Z","shell.execute_reply.started":"2026-02-24T09:47:56.131767Z","shell.execute_reply":"2026-02-24T09:47:56.145359Z"},"papermill":{"duration":0.023002,"end_time":"2026-02-20T09:51:35.193225","exception":false,"start_time":"2026-02-20T09:51:35.170223","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Liver Tumor Segmentation","metadata":{"papermill":{"duration":0.004972,"end_time":"2026-02-20T09:51:35.221708","exception":false,"start_time":"2026-02-20T09:51:35.216736","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Visualization Helpers","metadata":{"papermill":{"duration":0.005094,"end_time":"2026-02-20T09:51:35.262722","exception":false,"start_time":"2026-02-20T09:51:35.257628","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nclass HistoryLogger:\n    def __init__(self, tag: str, out_dir=\"/kaggle/working\"):\n        self.tag = tag\n        self.out_dir = out_dir\n        self.rows = []\n\n    def add(self, epoch: int, train_loss: float, val_dict: dict, lr: float, sec: float):\n        row = {\n            \"epoch\": epoch,\n            \"train_loss\": float(train_loss),\n            \"val_loss\": float(val_dict[\"loss\"]),\n            \"dice\": float(val_dict.get(\"dice\", np.nan)),\n            \"iou\": float(val_dict.get(\"iou\", np.nan)),\n            \"acc\": float(val_dict.get(\"acc\", np.nan)),\n            \"sens\": float(val_dict.get(\"sens\", np.nan)),\n            \"spec\": float(val_dict.get(\"spec\", np.nan)),\n            \"lr\": float(lr),\n            \"sec\": float(sec),\n        }\n        self.rows.append(row)\n\n    def to_df(self):\n        return pd.DataFrame(self.rows)\n\n    def save_csv(self):\n        df = self.to_df()\n        path = f\"{self.out_dir}/history_{self.tag}.csv\"\n        df.to_csv(path, index=False)\n        print(f\"saved {path}\")\n        return path\n\n    def plot(self):\n        df = self.to_df()\n        if len(df) == 0:\n            print(\"No history to plot.\")\n            return\n\n        # Loss\n        plt.figure()\n        plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"train_loss\")\n        plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"val_loss\")\n        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{self.tag} Loss\"); plt.legend()\n        plt.grid(True, alpha=0.3)\n        loss_path = f\"{self.out_dir}/plot_{self.tag}_loss.png\"\n        plt.savefig(loss_path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        print(f\"saved {loss_path}\")\n\n        # Dice/IoU\n        if \"dice\" in df.columns:\n            plt.figure()\n            plt.plot(df[\"epoch\"], df[\"dice\"], label=\"dice\")\n            if \"iou\" in df.columns:\n                plt.plot(df[\"epoch\"], df[\"iou\"], label=\"iou\")\n            plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(f\"{self.tag} Dice/IoU\"); plt.legend()\n            plt.grid(True, alpha=0.3)\n            met_path = f\"{self.out_dir}/plot_{self.tag}_metrics.png\"\n            plt.savefig(met_path, dpi=150, bbox_inches=\"tight\")\n            plt.show()\n            print(f\"saved {met_path}\")\n\n        # Accuracy/Sens/Spec\n        if \"acc\" in df.columns:\n            plt.figure()\n            plt.plot(df[\"epoch\"], df[\"acc\"], label=\"acc\")\n            if \"sens\" in df.columns:\n                plt.plot(df[\"epoch\"], df[\"sens\"], label=\"sens\")\n            if \"spec\" in df.columns:\n                plt.plot(df[\"epoch\"], df[\"spec\"], label=\"spec\")\n            plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(f\"{self.tag} Acc/Sens/Spec\"); plt.legend()\n            plt.grid(True, alpha=0.3)\n            cls_path = f\"{self.out_dir}/plot_{self.tag}_clsmetrics.png\"\n            plt.savefig(cls_path, dpi=150, bbox_inches=\"tight\")\n            plt.show()\n            print(f\"saved {cls_path}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:48:00.537438Z","iopub.execute_input":"2026-02-24T09:48:00.538188Z","iopub.status.idle":"2026-02-24T09:48:00.549707Z","shell.execute_reply.started":"2026-02-24T09:48:00.538160Z","shell.execute_reply":"2026-02-24T09:48:00.549017Z"},"papermill":{"duration":0.01965,"end_time":"2026-02-20T09:51:35.287386","exception":false,"start_time":"2026-02-20T09:51:35.267736","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import random\n\n@torch.no_grad()\ndef visualize_predictions(model, loader, device, n=6, thr=0.5, title=\"preds\"):\n    model.eval()\n    batch = next(iter(loader))\n    x, y = batch\n    x = x.to(device)\n    y = y.to(device)\n\n    logits = model(x)\n    prob = torch.sigmoid(logits)\n    pred = (prob > thr).float()\n\n    n = min(n, x.shape[0])\n    idxs = random.sample(range(x.shape[0]), n)\n\n    plt.figure(figsize=(12, 2*n))\n    for i, idx in enumerate(idxs):\n        # show first channel for visualization (liver window)\n        img = x[idx, 0].detach().cpu().numpy()\n        gt  = y[idx, 0].detach().cpu().numpy()\n        pr  = pred[idx, 0].detach().cpu().numpy()\n\n        # overlay prediction (red) + GT (green)\n        overlay = np.stack([img, img, img], axis=-1)\n        overlay[..., 0] = np.clip(overlay[..., 0] + 0.6*pr, 0, 1)\n        overlay[..., 1] = np.clip(overlay[..., 1] + 0.6*gt, 0, 1)\n\n        ax = plt.subplot(n, 3, 3*i + 1)\n        ax.imshow(img, cmap=\"gray\"); ax.set_title(\"image\"); ax.axis(\"off\")\n        ax = plt.subplot(n, 3, 3*i + 2)\n        ax.imshow(gt, cmap=\"gray\"); ax.set_title(\"gt\"); ax.axis(\"off\")\n        ax = plt.subplot(n, 3, 3*i + 3)\n        ax.imshow(overlay); ax.set_title(\"overlay (pred red, gt green)\"); ax.axis(\"off\")\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    out_path = f\"/kaggle/working/{title}.png\"\n    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"saved {out_path}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:48:04.938091Z","iopub.execute_input":"2026-02-24T09:48:04.938393Z","iopub.status.idle":"2026-02-24T09:48:04.947143Z","shell.execute_reply.started":"2026-02-24T09:48:04.938368Z","shell.execute_reply":"2026-02-24T09:48:04.946375Z"},"papermill":{"duration":0.016474,"end_time":"2026-02-20T09:51:35.308983","exception":false,"start_time":"2026-02-20T09:51:35.292509","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Making Cache","metadata":{"papermill":{"duration":0.005286,"end_time":"2026-02-20T09:51:35.319331","exception":false,"start_time":"2026-02-20T09:51:35.314045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_cache_LiverTumor_for_ids(volume_ids, split: str):\n    img_dir, msk_dir = cache_paths(split)\n    os.makedirs(img_dir, exist_ok=True)\n    os.makedirs(msk_dir, exist_ok=True)\n\n    print(f\"[{split}] Building LiverTumor cache ...\")\n    kept_pos = 0\n    kept_neg = 0\n\n    # Use neg probability only for training\n    keep_neg_prob = cfg.KEEP_NEGATIVE_PROB if split == \"train\" else 0.0\n\n    for vid in tqdm(volume_ids, desc=f\"Building {split} LiverTumor cache (volumes)\"):\n        ct = nib.load(ct_map[vid]).get_fdata(dtype=np.float32)     # (H,W,Z)\n        msk = nib.load(msk_map[vid]).get_fdata(dtype=np.float32)   # (H,W,Z)\n        Z = ct.shape[2]\n\n        for z in range(Z):\n            m = msk[:, :, z]\n\n            # LiverTumor foreground: any label > 0\n            fg = (m > 0).astype(np.float32)\n\n            is_positive = fg.sum() >= 20  # same threshold idea as you used\n\n            # Decide keep/skip\n            if not is_positive:\n                # negative slice (no liver/tumor)\n                if random.random() > keep_neg_prob:\n                    continue\n\n            stem = f\"id{vid:03d}_z{z:03d}\"\n\n            img = ct[:, :, z]\n            img3 = create_multi_channel_image(img)  # [H,W,3] in [0,1]\n            img3 = cv2.resize(img3, (cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n\n            fg_r = cv2.resize(fg, (cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n\n            np.save(os.path.join(img_dir, f\"{stem}.npy\"), img3.astype(np.float32))\n            np.save(os.path.join(msk_dir, f\"{stem}.npy\"), fg_r.astype(np.float32))\n\n            if is_positive:\n                kept_pos += 1\n            else:\n                kept_neg += 1\n\n    print(f\"[{split}] Done. Cached positives: {kept_pos} | Cached negatives: {kept_neg}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:48:07.330553Z","iopub.execute_input":"2026-02-24T09:48:07.331190Z","iopub.status.idle":"2026-02-24T09:48:07.338707Z","shell.execute_reply.started":"2026-02-24T09:48:07.331160Z","shell.execute_reply":"2026-02-24T09:48:07.338015Z"},"papermill":{"duration":0.014891,"end_time":"2026-02-20T09:51:35.339532","exception":false,"start_time":"2026-02-20T09:51:35.324641","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Training flow","metadata":{"papermill":{"duration":0.005278,"end_time":"2026-02-20T09:51:35.447262","exception":false,"start_time":"2026-02-20T09:51:35.441984","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# build liver cache\nbuild_cache_LiverTumor_for_ids(train_ids, \"train\")\nbuild_cache_LiverTumor_for_ids(val_ids, \"val\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T09:48:18.978326Z","iopub.execute_input":"2026-02-24T09:48:18.979044Z","iopub.status.idle":"2026-02-24T10:02:19.192258Z","shell.execute_reply.started":"2026-02-24T09:48:18.979006Z","shell.execute_reply":"2026-02-24T10:02:19.191543Z"},"papermill":{"duration":796.450849,"end_time":"2026-02-20T10:04:51.903283","exception":false,"start_time":"2026-02-20T09:51:35.452434","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[train] Building LiverTumor cache ...\n","output_type":"stream"},{"name":"stderr","text":"Building train LiverTumor cache (volumes): 100%|██████████| 88/88 [10:59<00:00,  7.49s/it]\n","output_type":"stream"},{"name":"stdout","text":"[train] Done. Cached positives: 12493 | Cached negatives: 1252\n[val] Building LiverTumor cache ...\n","output_type":"stream"},{"name":"stderr","text":"Building val LiverTumor cache (volumes): 100%|██████████| 23/23 [03:00<00:00,  7.86s/it]","output_type":"stream"},{"name":"stdout","text":"[val] Done. Cached positives: 3069 | Cached negatives: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"train_ds = NPYSliceDataset(cfg.CACHE_DIR, \"train\", augment=True)\nval_ds   = NPYSliceDataset(cfg.CACHE_DIR, \"val\", augment=False)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=cfg.BATCH_SIZE,\n    shuffle=True,\n    num_workers=cfg.NUM_WORKERS,\n    pin_memory=cfg.PIN_MEMORY,\n    persistent_workers=(cfg.NUM_WORKERS > 0),\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size=cfg.BATCH_SIZE,\n    shuffle=False,\n    num_workers=cfg.NUM_WORKERS,\n    pin_memory=cfg.PIN_MEMORY,\n    persistent_workers=(cfg.NUM_WORKERS > 0),\n)\n\nprint(f\"[LIVER Tumor] Cached train slices: {len(train_ds)} | Cached val slices: {len(val_ds)}\")\n# speed check\nt0 = time.time()\nfor i, (x, y) in enumerate(train_loader):\n    if i == 50:\n        break\nprint(\"50 batches load time:\", time.time() - t0, \"sec\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T10:31:17.771807Z","iopub.execute_input":"2026-02-24T10:31:17.772470Z","iopub.status.idle":"2026-02-24T10:31:21.206371Z","shell.execute_reply.started":"2026-02-24T10:31:17.772440Z","shell.execute_reply":"2026-02-24T10:31:21.205237Z"},"papermill":{"duration":5.011622,"end_time":"2026-02-20T10:04:56.924915","exception":false,"start_time":"2026-02-20T10:04:51.913293","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"Indexing train .npy slices: 100%|██████████| 13745/13745 [00:00<00:00, 4817071.23it/s]\nIndexing val .npy slices: 100%|██████████| 3069/3069 [00:00<00:00, 3897159.85it/s]","output_type":"stream"},{"name":"stdout","text":"[LIVER Tumor] Cached train slices: 13745 | Cached val slices: 3069\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"50 batches load time: 3.3808228969573975 sec\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"hist1 = HistoryLogger(\"LiverTumor\")\n\ndevice = torch.device(cfg.DEVICE)\nmodel = UNetBase(in_channels=3, out_channels=1, base=64).to(device)\n\npos_w = estimate_pos_weight(train_loader, device)\nloss_fn = BCEDice(pos_weight=pos_w, bce_weight=0.3).to(device)\n\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=5)\n\nbest_dice = -1.0\nEPOCHS = 40\n\nfor ep in range(1, EPOCHS + 1):\n    t0 = time.time()  # start timer\n\n    tr_loss = train_one_epoch(model, train_loader, opt, loss_fn, device)\n    val = evaluate(model, val_loader, loss_fn, device)\n\n    scheduler.step(val[\"dice\"])\n    lr = opt.param_groups[0][\"lr\"]   \n    dt = time.time() - t0            \n\n    if val[\"dice\"] > best_dice:\n        best_dice = val[\"dice\"]\n        torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"val\": val}, \"best_unet_LiverTumor.pt\")\n        print(f\"saved best_unet_LiverTumor.pt (best_dice={best_dice:.4f})\")\n\n    print(\n        f\"Epoch {ep:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={val['loss']:.4f} | \"\n        f\"dice={val['dice']:.4f} iou={val['iou']:.4f} acc={val['acc']:.4f} \"\n        f\"sens={val['sens']:.4f} spec={val['spec']:.4f} | lr={lr:.2e} | {dt:.1f}s\"\n    )\n\n    hist1.add(ep, tr_loss, val, lr, dt)\n\nhist1.save_csv()\nhist1.plot()\n\nck = torch.load(\"best_unet_LiverTumor.pt\", map_location=device)\nmodel.load_state_dict(ck[\"model\"])\nmodel.eval()\n\nvisualize_predictions(model, val_loader, device, n=6, thr=0.5, title=\"LiverTumor_preds\")","metadata":{"execution":{"iopub.status.busy":"2026-02-23T07:25:58.926121Z","iopub.execute_input":"2026-02-23T07:25:58.926954Z","iopub.status.idle":"2026-02-23T07:43:05.230092Z","shell.execute_reply.started":"2026-02-23T07:25:58.926909Z","shell.execute_reply":"2026-02-23T07:43:05.229073Z"},"papermill":{"duration":12070.073868,"end_time":"2026-02-20T13:26:07.012910","exception":false,"start_time":"2026-02-20T10:04:56.939042","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Download best_unet_MSLesion.pt from Google Drive\n# ============================================================\n\nimport os\nimport re\nimport subprocess\n\nGDRIVE_URL = \"https://drive.google.com/file/d/1kCiERrbDukSMnBhFfVqTF05GFQswv3dA/view?usp=sharing\"\n\n# Where to save\nOUT_PATH = \"/kaggle/working/best_unet_LiverTumor.pt\"\n\ndef extract_file_id(url: str):\n    # Works for /file/d/<id>/view links\n    m = re.search(r\"/file/d/([^/]+)/\", url)\n    if m:\n        return m.group(1)\n    # Works for id=<id> links\n    m = re.search(r\"[?&]id=([^&]+)\", url)\n    if m:\n        return m.group(1)\n    raise ValueError(\"Could not extract file ID from URL\")\n\nfile_id = extract_file_id(GDRIVE_URL)\nprint(\"Google Drive File ID:\", file_id)\n\n# Install gdown (if not installed)\nsubprocess.check_call([\"pip\", \"-q\", \"install\", \"gdown\"])\n\nimport gdown\n\nprint(\"Downloading model...\")\ngdown.download(id=file_id, output=OUT_PATH, quiet=False)\n\nprint(\"Download complete:\", OUT_PATH)\n\n# Optional: verify file exists\nif os.path.exists(OUT_PATH):\n    print(\"File size (MB):\", os.path.getsize(OUT_PATH)/1024/1024)\nelse:\n    print(\"Download failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T10:20:06.951947Z","iopub.execute_input":"2026-02-24T10:20:06.952667Z","iopub.status.idle":"2026-02-24T10:20:15.196942Z","shell.execute_reply.started":"2026-02-24T10:20:06.952619Z","shell.execute_reply":"2026-02-24T10:20:15.196328Z"}},"outputs":[{"name":"stdout","text":"Google Drive File ID: 1kCiERrbDukSMnBhFfVqTF05GFQswv3dA\nDownloading model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1kCiERrbDukSMnBhFfVqTF05GFQswv3dA\nFrom (redirected): https://drive.google.com/uc?id=1kCiERrbDukSMnBhFfVqTF05GFQswv3dA&confirm=t&uuid=4614341f-26c9-467b-9abf-852d39f9a6af\nTo: /kaggle/working/best_unet_LiverTumor.pt\n100%|██████████| 124M/124M [00:02<00:00, 58.3MB/s] ","output_type":"stream"},{"name":"stdout","text":"Download complete: /kaggle/working/best_unet_LiverTumor.pt\nFile size (MB): 118.3936128616333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# ============================================================\n# Inference on a single NIfTI slice + Visualization\n# ============================================================\n\nimport numpy as np\nimport cv2\nimport nibabel as nib\nimport torch\nimport matplotlib.pyplot as plt\n\n@torch.no_grad()\ndef run_inference(volume_path: str,\n                  ckpt_path: str,\n                  slice_idx: int,\n                  mask_path: str = None,\n                  img_size: int = None,\n                  thr: float = 0.5,\n                  device: torch.device = None,\n                  base: int = 64):\n    \"\"\"\n    Load a trained UNetBase checkpoint and run inference on one CT slice.\n\n    Notes\n    -----\n    - This assumes you trained the model with:\n        * UNetBase(in_channels=3, out_channels=1, base=64)\n        * create_multi_channel_image(...) preprocessing\n        * resizing to cfg.IMG_SIZE (default 256)\n    - If you change any of those settings during training, update them here too.\n\n    Parameters\n    ----------\n    volume_path : str\n        Path to CT volume (e.g., volume-XXX.nii).\n    ckpt_path : str\n        Path to saved checkpoint (e.g., 'best_unet_LiverTumor.pt').\n    slice_idx : int\n        Axial slice index (0..Z-1). Negative values work like Python indexing.\n    mask_path : str, optional\n        Path to GT mask (e.g., segmentation-XXX.nii) to visualize ground truth (mask > 0).\n        If None, only the prediction will be shown.\n    img_size : int, optional\n        Spatial size for the network input. Defaults to cfg.IMG_SIZE if present, else 256.\n    thr : float\n        Threshold on sigmoid probability to produce binary mask.\n    device : torch.device, optional\n        If None, uses CUDA if available.\n    base : int\n        UNetBase base feature size (must match training).\n\n    Returns\n    -------\n    vis_img : np.ndarray\n        2D image used for display (channel 0 = liver window), shape [img_size, img_size].\n    pred_mask : np.ndarray\n        Binary predicted mask (0/1), shape [img_size, img_size].\n    bg_mask : np.ndarray\n        Background mask (1 - pred_mask), shape [img_size, img_size].\n    \"\"\"\n    # --- basic config ---\n    if img_size is None:\n        img_size = cfg.IMG_SIZE if \"cfg\" in globals() else 256\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Running inference on: {device}\")\n\n    # --- Build model (MUST match training settings) ---\n    model = UNetBase(in_channels=3, out_channels=1, base=base).to(device)\n\n    # --- Load checkpoint robustly (supports several common formats) ---\n    ckpt = torch.load(ckpt_path, map_location=device)\n\n    state_dict = None\n    if isinstance(ckpt, dict):\n        # Your training loop saves {\"model\": state_dict, ...}\n        state_dict = ckpt.get(\"model\") or ckpt.get(\"model_state_dict\") or ckpt.get(\"state_dict\")\n\n    if state_dict is None:\n        state_dict = ckpt  # raw state_dict case\n\n    # Handle DataParallel \"module.\" prefix if present\n    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n\n    model.load_state_dict(state_dict, strict=True)\n    model.eval()\n\n    # --- Load CT slice ---\n    ct = nib.load(volume_path).get_fdata(dtype=np.float32)  # LiTS: (H,W,Z)\n    Z = ct.shape[2]\n\n    if slice_idx < 0:\n        slice_idx = Z + slice_idx\n    if not (0 <= slice_idx < Z):\n        raise ValueError(f\"slice_idx={slice_idx} out of range for Z={Z}\")\n\n    ct_slice = ct[:, :, slice_idx]\n\n    # --- Preprocess exactly like training ---\n    # create_multi_channel_image must be defined earlier in the notebook.\n    x3 = create_multi_channel_image(ct_slice)  # [H,W,3] in [0,1]\n    x3 = cv2.resize(x3, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n\n    # tensor: (B,C,H,W)\n    x = torch.from_numpy(x3).permute(2, 0, 1).unsqueeze(0).to(device)\n\n    # --- Predict ---\n    logits = model(x)\n    prob = torch.sigmoid(logits).squeeze(0).squeeze(0).cpu().numpy()\n    pred_mask = (prob >= thr).astype(np.uint8)\n\n    # --- Optional ground truth mask (mask > 0 = liver + tumor) ---\n    true_mask = None\n    if mask_path is not None:\n        m = nib.load(mask_path).get_fdata(dtype=np.float32)[:, :, slice_idx]\n        true_mask = (m > 0).astype(np.uint8)\n        true_mask = cv2.resize(true_mask, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n\n    # For visualization, show the \"liver window\" channel (channel 0)\n    vis_img = x3[..., 0]\n\n    # --- Plot ---\n    ncols = 3 if true_mask is not None else 2\n    fig, axes = plt.subplots(1, ncols, figsize=(5 * ncols, 5))\n\n    axes[0].imshow(vis_img, cmap=\"gray\")\n    axes[0].set_title(f\"CT (slice {slice_idx})\")\n    axes[0].axis(\"off\")\n\n    if true_mask is not None:\n        axes[1].imshow(vis_img, cmap=\"gray\")\n        axes[1].imshow(true_mask, cmap=\"Greens\", alpha=0.5)\n        axes[1].set_title(\"Ground truth (green)\")\n        axes[1].axis(\"off\")\n        ax_pred = axes[2]\n    else:\n        ax_pred = axes[1]\n\n    ax_pred.imshow(vis_img, cmap=\"gray\")\n    ax_pred.imshow(pred_mask, cmap=\"Reds\", alpha=0.5)\n    ax_pred.set_title(f\"Prediction (red) | thr={thr}\")\n    ax_pred.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n    bg_mask = 1 - pred_mask\n    return vis_img, pred_mask, bg_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T10:20:29.101630Z","iopub.execute_input":"2026-02-24T10:20:29.102128Z","iopub.status.idle":"2026-02-24T10:20:29.116207Z","shell.execute_reply.started":"2026-02-24T10:20:29.102102Z","shell.execute_reply":"2026-02-24T10:20:29.115256Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# ============================================================\n# Validation Inference + Average Dice and Accuracy\n# ============================================================\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nCKPT_PATH = \"best_unet_LiverTumor.pt\"   # <-- change if your checkpoint name/path differs\nTHR = 0.5                               # sigmoid threshold\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# --- Load model ---\nmodel = UNetBase(in_channels=3, out_channels=1, base=64).to(device)\n\nckpt = torch.load(CKPT_PATH, map_location=device)\n\n# handle different checkpoint formats\nif isinstance(ckpt, dict) and (\"model\" in ckpt or \"model_state_dict\" in ckpt or \"state_dict\" in ckpt):\n    state_dict = ckpt.get(\"model\") or ckpt.get(\"model_state_dict\") or ckpt.get(\"state_dict\")\nelse:\n    state_dict = ckpt  # raw state_dict\n\n# handle DataParallel prefix just in case\nstate_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n\nmodel.load_state_dict(state_dict, strict=True)\nmodel.eval()\n\n\n@torch.no_grad()\ndef eval_val_loader(model, loader, device, thr=0.5, eps=1e-8):\n    \"\"\"\n    Returns:\n      dice_macro: mean Dice over samples (per-slice average)\n      acc_macro:  mean pixel accuracy over samples\n      dice_micro: Dice computed from total TP/FP/FN over whole val set\n      acc_micro:  pixel accuracy computed from total TP/TN/FP/FN over whole val set\n    \"\"\"\n    dice_sum = 0.0\n    acc_sum  = 0.0\n    n_samples = 0\n\n    TP = 0.0\n    TN = 0.0\n    FP = 0.0\n    FN = 0.0\n\n    for x, y in tqdm(loader, desc=\"Running val inference\"):\n        x = x.to(device, non_blocking=True)  # [B,3,H,W]\n        y = y.to(device, non_blocking=True)  # [B,1,H,W]\n\n        logits = model(x)                    # [B,1,H,W]\n        prob = torch.sigmoid(logits)\n        pred = (prob >= thr).float()\n\n        # per-sample confusion terms (sum over C,H,W)\n        tp = (pred * y).sum(dim=(1,2,3))\n        tn = ((1 - pred) * (1 - y)).sum(dim=(1,2,3))\n        fp = (pred * (1 - y)).sum(dim=(1,2,3))\n        fn = ((1 - pred) * y).sum(dim=(1,2,3))\n\n        # per-sample Dice + Acc (macro averaging)\n        den = (2*tp + fp + fn)\n        dice = torch.where(den > 0, (2*tp + eps) / (den + eps), torch.ones_like(den))\n        acc  = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n\n        bs = x.size(0)\n        dice_sum += dice.sum().item()\n        acc_sum  += acc.sum().item()\n        n_samples += bs\n\n        # accumulate totals for micro/global metrics\n        TP += tp.sum().item()\n        TN += tn.sum().item()\n        FP += fp.sum().item()\n        FN += fn.sum().item()\n\n    dice_macro = dice_sum / max(n_samples, 1)\n    acc_macro  = acc_sum  / max(n_samples, 1)\n\n    dice_micro = (2*TP) / (2*TP + FP + FN + eps)\n    acc_micro  = (TP + TN) / (TP + TN + FP + FN + eps)\n\n    return {\n        \"dice_macro\": dice_macro,\n        \"acc_macro\": acc_macro,\n        \"dice_micro\": dice_micro,\n        \"acc_micro\": acc_micro,\n        \"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN,\n        \"n_samples\": n_samples\n    }\n\n\n# --- Run evaluation ---\nmetrics = eval_val_loader(model, val_loader, device, thr=THR)\n\nprint(\"\\n===== Validation Metrics =====\")\nprint(f\"Samples (slices): {metrics['n_samples']}\")\nprint(f\"Dice (macro avg):  {metrics['dice_macro']:.4f}\")\nprint(f\"Acc  (macro avg):  {metrics['acc_macro']:.4f}\")\nprint(f\"Dice (micro/global): {metrics['dice_micro']:.4f}\")\nprint(f\"Acc  (micro/global): {metrics['acc_micro']:.4f}\")\nprint(f\"TP={metrics['TP']:.0f} TN={metrics['TN']:.0f} FP={metrics['FP']:.0f} FN={metrics['FN']:.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T10:20:31.517734Z","iopub.execute_input":"2026-02-24T10:20:31.518433Z","iopub.status.idle":"2026-02-24T10:20:55.648257Z","shell.execute_reply.started":"2026-02-24T10:20:31.518401Z","shell.execute_reply":"2026-02-24T10:20:55.647326Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Running val inference: 100%|██████████| 192/192 [00:23<00:00,  8.09it/s]","output_type":"stream"},{"name":"stdout","text":"\n===== Validation Metrics =====\nSamples (slices): 3069\nDice (macro avg):  0.9129\nAcc  (macro avg):  0.9938\nDice (micro/global): 0.9554\nAcc  (micro/global): 0.9938\nTP=13400452 TN=186479875 FP=893579 FN=356078\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":41}]}